{"fielname": "visual_instruction_tunning.pdf", "contenido": "Visual Instruction Tuning\nHaotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\n1University of Wisconsin–Madison2Microsoft Research3Columbia University\nhttps://llava-vl.github.io\nAbstract\nInstruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has been shown to improve zero-shot capabilities on\nnew tasks, but the idea is less explored in the multimodal field. We present the\nfirst attempt to use language-only GPT-4 to generate multimodal language-image\ninstruction-following data. By instruction tuning on such generated data, we in-\ntroduce LLaV A: Large Language andVision Assistant, an end-to-end trained\nlarge multimodal model that connects a vision encoder and an LLM for general-\npurpose visual and language understanding. To facilitate future research on visual\ninstruction following, we construct two evaluation benchmarks with diverse and\nchallenging application-oriented tasks. Our experiments show that LLaV A demon-\nstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\nof multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\ntive score compared with GPT-4 on a synthetic multimodal instruction-following\ndataset. When fine-tuned on Science QA, the synergy of LLaV A and GPT-4\nachieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\nvisual instruction tuning data, our model, and code publicly available.\n1 Introduction\nHumans interact with the world through many channels such as vision and language, as each\nindividual channel has a unique advantage in representing and communicating certain concepts, and\nthus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\nis to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\ninstructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\nTo this end, the community has witnessed an emergent interest in developing language-augmented\nfoundation vision models [ 27,16], with strong capabilities in open-world visual understanding\nsuch as classification [ 40,21,57,54,39], detection [ 29,62,33], segmentation [ 25,63,58] and\ncaptioning [ 50,28], as well as visual generation and editing [ 42,43,56,15,44,30]. We refer readers\nto the Computer Vision in the Wild reading list for a more up-to-date literature compilation [ 12]. In\nthis line of work, each task is solved independently by one single large vision model, with the task\ninstruction implicitly considered in the model design. Further, language is only utilized to describe\nthe image content. While this allows language to play an important role in mapping visual signals to\nlanguage semantics—a common channel for human communication, it leads to models that usually\nhave a fixed interface with limited interactivity and adaptability to the user’s instructions.\nLarge language models (LLM), on the other hand, have shown that language can play a wider\nrole: a universal interface for a general-purpose assistant, where various task instructions can be\nexplicitly represented in language and guide the end-to-end trained neural assistant to switch to the\ntask of interest to solve it. For example, the recent success of ChatGPT [ 35] and GPT-4 [ 36] have\ndemonstrated the power of aligned LLMs in following human instructions, and have stimulated\ntremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open-\nsource LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [ 9], GPT-4-LLM [ 38]\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2304.08485v2  [cs.CV]  11 Dec 2023utilize various machine-generated high-quality instruction-following samples to improve the LLM’s\nalignment ability, reporting impressive performance compared with proprietary LLMs. Importantly,\nthis line of work is text-only .\nIn this paper, we present visual instruction-tuning , the first attempt to extend instruction-tuning to\nthe language-image multimodal space, to pave the way towards building a general-purpose visual\nassistant. In particular, our paper makes the following contributions:\n•Multimodal instruction-following data . One key challenge is the lack of vision-language\ninstruction-following data. We present a data reformation perspective and pipeline to convert\nimage-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4.\n•Large multimodal models . We develop a large multimodal model (LMM), by connecting the\nopen-set visual encoder of CLIP [ 40] with the language decoder Vicuna [ 9], and fine-tuning\nend-to-end on our generated instructional vision-language data. Our empirical study validates\nthe effectiveness of using generated data for LMM instruction-tuning, and suggests practical\ntips for building a general-purpose instruction-following visual agent. When ensembled with\nGPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset.\n•Multimodal instruction-following benchmark . We present LLaV A-Bench with two challenging\nbenchmarks, with a diverse selection of paired images, instructions and detailed annotations.\n•Open-source . We release the following assets to the public: the generated multimodal instruction\ndata, the codebase, the model checkpoints, and a visual chat demo.\n2 Related Work\nMultimodal Instruction-following Agents. In computer vision, existing works that build instruction-\nfollowing agents can be broadly categorized into two classes: (i)End-to-end trained models, which\nare separately explored for each specific research topic. For example, the vision-language navigation\ntask [ 3,19] and Habitat [ 47] require the embodied AI agent to follow natural language instructions\nand take a sequence of actions to complete goals in visual environments. In the image editing domain,\ngiven an input image and a written instruction that tells the agent what to do, InstructPix2Pix [ 6]\nedits images by following the human instructions. (ii)A system that coordinates various models\nvia LangChain [ 1] / LLMs [ 35], such as Visual ChatGPT [ 53], X-GPT [ 63], MM-REACT [ 55],\nVisProg [ 18], and ViperGPT [ 46]. While sharing the same goal in building instruction-following\nagents, we focus on developing an end-to-end trained language-vision multimodal model for multiple\ntasks.\nInstruction Tuning. In the natural language processing (NLP) community, to enable LLMs such\nas GPT-3 [ 7], T5 [ 41], PaLM [ 10], and OPT [ 60] to follow natural language instructions and\ncomplete real-world tasks, researchers have explored methods for LLM instruction-tuning [ 37,52,51],\nleading to instruction-tuned counterparts such as InstructGPT [ 37]/ChatGPT [ 35], FLAN-T5 [ 11],\nFLAN-PaLM [ 11], and OPT-IML [ 22], respectively. It turns out that this simple approach can\neffectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural\nto borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation\nideas with foundation models have been studied in other topics such as image classification [ 14].\nFlamingo [ 2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong\nperformance on zero-shot task transfer and in-context-learning. Other LMMs trained on image-\ntext pairs include BLIP-2 [ 28], FROMAGe [ 24], and KOSMOS-1 [ 20]. PaLM-E [ 13] is an LMM\nfor embodied AI. Based on the recent “best” open-source LLM LLaMA, OpenFlamingo [ 5] and\nLLaMA-Adapter [ 59] are open-source efforts that enable LLaMA to use image inputs, paving the\nway to build open-source multimodal LLMs. While these models present promising task transfer\ngeneralization performance, they are not explicitly tuned with vision-language instruction data, and\ntheir performance in multimodal tasks usually falls short compared to language-only tasks. In this\npaper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning\nis different from visual prompt tuning [ 23]: the former aims to improve the model’s instruction-\nfollowing abilities, while the latter aims to improve the parameter-efficiency in model adaptation.\n3 GPT-assisted Visual Instruction Data Generation\nThe community has witnessed a surge in the amount of public multimodal data such as image-text\npairs, ranging from CC [ 8] to LAION [ 45]. However, when it comes to multimodal instruction-\n2Context type 1: Captions\nA group of people standing outside of a black vehicle with various luggage.\nLuggage surrounds a vehicle in an underground parking area\nPeople try to fit all of their luggage in an SUV .\nThe sport utility vehicle is parked in the public garage, being packed for a trip\nSome people with luggage near a van that is transporting it.\nContext type 2: Boxes\nperson: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted>\nResponse type 1: conversation\nQuestion: What type of vehicle is featured in the image?\nAnswer: The image features a black sport utility vehicle (SUV) ...<omitted>\nResponse type 2: detailed description\nThe image is an underground parking area with a black sport utility vehicle (SUV) parked. There\nare three people in the scene, with one person standing closer to the left side of the vehicle, another\nperson in the middle, and the third person on the right side. They are all working together to pack\ntheir luggage into the SUV for a trip. ...<omitted>\nResponse type 3: complex reasoning\nQuestion: What challenges do these people face?\nAnswer: In the image, a group of people is standing outside a black SUV in a parking area,\nsurrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\nchallenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks\nto be packed, which suggests that the group has a significant amount of belongings ...<omitted>\nTable 1: One example to illustrate the instruction-following data. The top block shows the contexts\nsuch as captions and boxes used to prompt GPT, and the bottom block shows the three types of\nresponses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\nfollowing data, the available amount is limited, partially because the process for creating such data is\ntime-consuming and less well-defined when human crowd-scouring is considered. Inspired by the\nsuccess of recent GPT models in text-annotation tasks [ 17], we propose to leverage ChatGPT/GPT-4\nfor multimodal instruction-following data collection, based on the widely existing image-pair data.\nFor an image Xvand its associated caption Xc, it is natural to create a set of questions Xqwith the\nintent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list\nof questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its\ninstruction-following version is Human :XqXv<STOP> Assistant :Xc<STOP> . Though cheap to\nconstruct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions\nand responses.\nTo mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both\naccept only text as input), to create instruction-following data involving visual content. Specifically,\nin order to encode an image into its visual features to prompt a text-only GPT, we use two types of\nsymbolic representations: (i)Captions typically describe the visual scene from various perspectives;\n(ii)Bounding boxes usually localize the objects in the scene, and each box encodes the object concept\nand its spatial location. One example is shown in the top block of Table 14.\nThis symbolic representation allows us to encode the image as an LLM-recognizable sequence. We\nuse COCO images [ 31] and generate three types of instruction-following data. One example per type\nis shown in the bottom block of Table 14. For each type, we first manually design a few examples.\nThey are the only human annotations we have during data collection, and are used as seed examples\nin in-context-learning to query GPT-4.\n•Conversation . We design a conversation between the assistant and a person asking questions\nabout this photo. The answers are in a tone as if the assistant is seeing the image and answering\nthe question. A diverse set of questions are asked about the visual content of the image, including\nthe object types, counting the objects, object actions, object locations, relative positions between\nobjects. Only questions that have definite answers are considered. Please see Appendix for the\ndetailed prompt.\n•Detailed description . To include a rich and comprehensive description for an image, we create a\nlist of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts\n3and curation process in Appendix). For each image, we randomly sample one question from the\nlist to ask GPT-4 to generate the detailed description.\n•Complex reasoning . The above two types focus on the visual content itself, based on which\nwe further create in-depth reasoning questions. The answers typically require a step-by-step\nreasoning process by following rigorous logic.\nWe collect 158K unique language-image instruction-following samples in total, including 58K in\nconversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated\nthe use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides\nhigher quality instruction-following data, such as spatial reasoning.\n4 Visual Instruction Tuning\n4.1 Architecture\nThe primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual\nmodel. The network archtecture is illustrated in Figure 1. We choose Vicuna [ 9] as our LLM fϕ(·)\nparameterized by ϕ, as it has the best instruction following capabilities in language tasks among\npublicly available checkpoints [48, 9, 38].\nVision Encoder<latexit sha1_base64=\"nmaulJAcZ9L9s1EtmepKU/wnbmw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUASwuykNxkzO7PMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHeFieDG+v63t7K6tr6xWdgqbu/s7u2XDg4bRqWaYZ0poXQrpAYFl1i33ApsJRppHApshqPbqd98Qm24kg92nGA3pgPJI86odVKjE0ZZc9Irlf2KPwNZJkFOypCj1it9dfqKpTFKywQ1ph34ie1mVFvOBE6KndRgQtmIDrDtqKQxmm42u3ZCTp3SJ5HSrqQlM/X3REZjY8Zx6Dpjaodm0ZuK/3nt1EbX3YzLJLUo2XxRlApiFZm+TvpcI7Ni7AhlmrtbCRtSTZl1ARVdCMHiy8ukcV4JLivB/UW5epPHUYBjOIEzCOAKqnAHNagDg0d4hld485T34r17H/PWFS+fOYI/8D5/AKc0jy8=</latexit>W<latexit sha1_base64=\"Dpm7JzZPmwmdKOQRfbESjXZnclA=\">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gOaWDbbTbt0dxN2N0oJ+R9ePCji1f/izX/jts1BWx8MPN6bYWZemHCmjet+O6WV1bX1jfJmZWt7Z3evun/Q1nGqCG2RmMeqG2JNOZO0ZZjhtJsoikXIaScc30z9ziNVmsXy3kwSGgg8lCxiBBsrPUT9zA+jDPnJiOV5v1pz6+4MaJl4BalBgWa/+uUPYpIKKg3hWOue5yYmyLAyjHCaV/xU0wSTMR7SnqUSC6qDbHZ1jk6sMkBRrGxJg2bq74kMC60nIrSdApuRXvSm4n9eLzXRVZAxmaSGSjJfFKUcmRhNI0ADpigxfGIJJorZWxEZYYWJsUFVbAje4svLpH1W9y7q3t15rXFdxFGGIziGU/DgEhpwC01oAQEFz/AKb86T8+K8Ox/z1pJTzBzCHzifP63EkqI=</latexit>f\u0000Projection<latexit sha1_base64=\"w6zXykpCeX38FkkjKhaO+oe7y2E=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUATwmQ6aYdOJmHmplhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57wlRwDY7zbS0tr6yurZc2yptb2zu79t5+UyeZoqxBE5Godkg0E1yyBnAQrJ0qRuJQsFY4uBn7rSFTmifyHkYp82PSkzzilICRAvsw98IIt4sg94A9AEA+LIrArjhVZwK8SNwZqaAZ6oH95XUTmsVMAhVE647rpODnRAGnghVlL9MsJXRAeqxjqCQx034+ub7AJ0bp4ihRpiTgifp7Iiex1qM4NJ0xgb6e98bif14ng+jKz7lMM2CSThdFmcCQ4HEUuMsVoyBGhhCquLkV0z5RhIIJrGxCcOdfXiTNs6p7UXXvziu161kcJXSEjtEpctElqqFbVEcNRNEjekav6M16sl6sd+tj2rpkzWYO0B9Ynz9pXpXb</latexit>Xv\n<latexit sha1_base64=\"OUBGioCbGiTqe2ceR2O0Z900t+I=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7AObECbTSTt08mDmplhD8FfcuFDErf/hzr9x2mahrQcuHM65l3vv8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3zN29lopTSVmTxiKWHZ8oJnjEmsBBsE4iGQl9wdr+8Hrit0dMKh5HdzBOmBuSfsQDTgloyTMPMscP8H3uZQ6wBwDIRnnumVWrZk2BF4ldkCoq0PDML6cX0zRkEVBBlOraVgJuRiRwKlhecVLFEkKHpM+6mkYkZMrNptfn+FgrPRzEUlcEeKr+nshIqNQ49HVnSGCg5r2J+J/XTSG4dDMeJSmwiM4WBanAEONJFLjHJaMgxpoQKrm+FdMBkYSCDqyiQ7DnX14krdOafV6zb8+q9asijjI6REfoBNnoAtXRDWqgJqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHbIKV3Q==</latexit>Zv<latexit sha1_base64=\"yn0ZUZSqKssiKPDZqlv9C0+1rLg=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5MHMTbGG4K+4caGIW//DnX/jtM1CWw9cOJxzL/fe4yeCK7Csb6O0srq2vlHerGxt7+zumfsHbRWnkrIWjUUsuz5RTPCItYCDYN1EMhL6gnX80e3U74yZVDyO7mGSMDckg4gHnBLQkmceZY4f4EbuZQ6wBwDIxnnumVWrZs2Al4ldkCoq0PTML6cf0zRkEVBBlOrZVgJuRiRwKlhecVLFEkJHZMB6mkYkZMrNZtfn+FQrfRzEUlcEeKb+nshIqNQk9HVnSGCoFr2p+J/XSyG4djMeJSmwiM4XBanAEONpFLjPJaMgJpoQKrm+FdMhkYSCDqyiQ7AXX14m7fOafVmz7y6q9ZsijjI6RifoDNnoCtVRAzVRC1H0iJ7RK3oznowX4934mLeWjGLmEP2B8fkDUD6Vyw==</latexit>HvImageLanguage InstructionLanguage Response <latexit sha1_base64=\"/KN5R7NUwEKH6XBR4DKeLzGzIrU=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5OHMjVhD8FfcuFDErf/hzr9x2mahrQcuHM65l3vv8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3zN29topTSVmLxiKWXZ8oJnjEWsBBsG4iGQl9wTr+6Hrid+6ZVDyObmGcMDckg4gHnBLQkmceZI4f4EbuZQ6wBwDI7vLcM6tWzZoCLxK7IFVUoOmZX04/pmnIIqCCKNWzrQTcjEjgVLC84qSKJYSOyID1NI1IyJSbTa/P8bFW+jiIpa4I8FT9PZGRUKlx6OvOkMBQzXsT8T+vl0Jw6WY8SlJgEZ0tClKBIcaTKHCfS0ZBjDUhVHJ9K6ZDIgkFHVhFh2DPv7xI2qc1+7xm35xV61dFHGV0iI7QCbLRBaqjBmqiFqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHSKCVxg==</latexit>Hq\n<latexit sha1_base64=\"4a/5KuBhqFrRimsGds8xVP6ZkkY=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUAbwmQ6aYdOJnHmRqwh+CtuXCji1v9w5984bbPQ1gMXDufcy733BIngGhzn21pYXFpeWS2tldc3Nre27Z3dpo5TRVmDxiJW7YBoJrhkDeAgWDtRjESBYK1geDX2W/dMaR7LWxglzItIX/KQUwJG8u39rBuEuJ37WRfYAwBkd3nu2xWn6kyA54lbkAoqUPftr24vpmnEJFBBtO64TgJeRhRwKlhe7qaaJYQOSZ91DJUkYtrLJtfn+MgoPRzGypQEPFF/T2Qk0noUBaYzIjDQs95Y/M/rpBBeeBmXSQpM0umiMBUYYjyOAve4YhTEyBBCFTe3YjogilAwgZVNCO7sy/OkeVJ1z6ruzWmldlnEUUIH6BAdIxedoxq6RnXUQBQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBYcCV1g==</latexit>Xq<latexit sha1_base64=\"I8RxJE902anMmciAczfxKVfe1PY=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7APaECbTSTt0MgkzN2INwV9x40IRt/6HO//GaZuFth64cDjnXu69J0gE1+A431ZpaXllda28XtnY3NresXf3WjpOFWVNGotYdQKimeCSNYGDYJ1EMRIFgrWD0fXEb98zpXks72CcMC8iA8lDTgkYybcPsl4Q4k7uZz1gDwCQkTz37apTc6bAi8QtSBUVaPj2V68f0zRiEqggWnddJwEvIwo4FSyv9FLNEkJHZMC6hkoSMe1l0+tzfGyUPg5jZUoCnqq/JzISaT2OAtMZERjqeW8i/ud1UwgvvYzLJAUm6WxRmAoMMZ5EgftcMQpibAihiptbMR0SRSiYwComBHf+5UXSOq255zX39qxavyriKKNDdIROkIsuUB3doAZqIooe0TN6RW/Wk/VivVsfs9aSVczsoz+wPn8ASWCVxg==</latexit>XaLanguage Model\nFigure 1: LLaV A network architecture.\nFor an input image Xv, we consider the pre-trained CLIP visual encoder ViT-L/14 [ 40], which\nprovides the visual feature Zv=g(Xv). The grid features before and after the last Transformer layer\nare considered in our experiments. We consider a simple linear layer to connect image features into\nthe word embedding space. Specifically, we apply a trainable projection matrix Wto convert Zvinto\nlanguage embedding tokens Hv, which have the same dimensionality as the word embedding space\nin the language model:\nHv=W·Zv,withZv=g(Xv) (1)\nThus, we have a sequence of visual tokens Hv. Note that our simple projection scheme is lightweight,\nwhich allows us to iterate data centric experiments quickly. More sophisticated schemes to con-\nnect the image and language representations can also be considered, such as gated cross-attention\nin Flamingo [ 2] and Q-former in BLIP-2 [ 28]. We leave exploring possibly more effective and\nsophisticated architecture designs for LLaV A as future work.\n4.2 Training\nFor each image Xv, we generate multi-turn conversation data (X1\nq,X1\na,···,XT\nq,XT\na), where Tis\nthe total number of turns. We organize them as a sequence, by treating all answers as the assistant’s\nresponse, and the instruction Xt\ninstruct at the t-th turn as:\nXt\ninstruct =\u001aRandomly choose [X1\nq,Xv]or[Xv,X1\nq],the first turn t= 1\nXt\nq, the remaining turns t >1(2)\nThis leads to the unified format for the multimodal instruction-following sequence illustrated in\nTable 2. We perform instruction-tuning of the LLM on the prediction tokens, using its original\nauto-regressive training objective.\nSpecifically, for a sequence of length L, we compute the probability of the target answers Xaby:\np(Xa|Xv,Xinstruct ) =LY\ni=1pθ(xi|Xv,Xinstruct ,<i,Xa,<i), (3)\n4Xsystem-message <STOP>\nHuman :X1\ninstruct <STOP> Assistant :X1\na<STOP>\nHuman :X2\ninstruct <STOP> Assistant :X2\na<STOP> · · ·\nTable 2: The input sequence used to train the model. Only two conversation turns are illustrated\nhere; in practice, the number of turns varies based on the instruction-following data. In our current\nimplementation, we follow Vicuna-v0 [ 9] to set the system message Xsystem-message and we set\n<STOP> =###. The model is trained to predict the assistant answers and where to stop, and thus\nonly green sequence/tokens are used to compute the loss in the auto-regressive model.\nwhere θis the trainable parameters, Xinstruct ,<iandXa,<iare the instruction and answer tokens in\nall turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of\nthe prediction tokens. For the conditionals in (3), we explicitly add Xvto emphasize the fact that the\nimage is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better\nreadability. For LLaV A model training, we consider a two-stage instruction-tuning procedure.\nStage 1: Pre-training for Feature Alignment. To strike a balance between concept coverage\nand training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details\nof the filtering process. These pairs are converted to the instruction-following data using the naive\nexpansion method describe in Section 3. Each sample can be treated as a single-turn conversation. To\nconstruct the input Xinstruct in(2), for an image Xv, a question Xqis randomly sampled, which is a\nlanguage instruction to request the assistant to describe the image briefly. The ground-truth prediction\nanswer Xais the original caption. In training, we keep both the visual encoder and LLM weights\nfrozen, and maximize the likelihood of (3)with trainable parameters θ=W(the projection matrix)\nonly. In this way, the image features Hvcan be aligned with the pre-trained LLM word embedding.\nThis stage can be understood as training a compatible visual tokenizer for the frozen LLM.\nStage 2: Fine-tuning End-to-End. We always keep the visual encoder weights frozen, and continue\nto update both the pre-trained weights of the projection layer and LLM in LLaV A; i.e., the trainable\nparameters are θ={W,ϕ}in (3). We consider two specific use case scenarios:\n•Multimodal Chatbot . We develop a Chatbot by fine-tuning on the 158K language-image\ninstruction-following data in Section 3. Among the three types of responses, conversation is\nmulti-turn while the other two are single-turn. They are uniformly sampled in training.\n•Science QA . We study our method on the ScienceQA benchmark [ 34], the first large-scale\nmultimodal science question dataset that annotates the answers with detailed lectures and\nexplanations. Each question is provided a context in the form of natural language or an image.\nThe assistant provides the reasoning process in natural language and selects the answer among\nmultiple choices. For training in (2), we organize the data as a single turn conversation, the\nquestion & context as Xinstruct , and reasoning & answer as Xa.\n5 Experiments\nWe assess the performance of LLaV A in instruction-following and visual reasoning capabilities with\ntwo primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We\ntrain all models with 8 ×A100s, following Vicuna’s hyperparameters [ 9]. We pre-train our model\non the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and\nfine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and\na batch size of 32. See Appendix for more training details.\n5.1 Multimodal Chatbot\nWe developed a chatbot demo to show the image understanding and conversation abilities of LLaV A,\nand to study how well LLaV A is able to digest visual inputs and exhibit instruction-following\ncapabilities. We first use the examples in the original GPT-4 paper [ 36], shown in Table 3 (more\nexamples in Appendix), that require in-depth image understanding. For comparisons, we quote the\nprompt and response of the multimodal GPT-4 from their paper, and query BLIP-2 and OpenFlamingo\nmodel checkpoints to get their response.\n5Visual input example, Extreme Ironing:\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg\nUser What is unusual about this image?\nLLaV A The unusual aspect of this image is a man ironing clothes on the back of a minivan or\nvan. This is not a typical place to perform this activity, as one would usually iron clothes\nin a more stationary and safe location, such as a home, using a regular ironing board.\nThe scene depicted in the image is peculiar as it involves a makeshift ironing setup on a\nvehicle, which can be both unsafe and unconventional. Additionally, it is not clear how\nthe man is able to maintain balance and stability while ironing clothes in such an unstable\nenvironment.\n[Start a new conversation, and clear the history ]\nUser What’s happening in the scene?\nLLaV A The image depicts a man standing on top of a yellow SUV in a busy city street. He is\nholding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around\nthe scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are\ntwo other people in the vicinity, one close to the man on the car and another further away.\nThe man on the ladder appears to be attracting attention due to his unconventional choice\nof ironing his clothes on top of a moving car. The city street around him is bustling with\nactivity, adding to the unique nature of the scene.\nUser What is unusual about this image?\nGPT-4 [36] The unusual thing about this image is that a man is ironing clothes on an ironing board\nattached to the roof of a moving taxi.\nUser What is unusual about this image?\nBLIP-2 a man is sitting on the back of a yellow cab\nUser What is unusual about this image?\nOpenFlamingo The man is drying his clothes on the hood of his car.\nTable 3: Example prompt from GPT-4 paper [ 36] to compare visual reasoning and chat capabilities.\nCompared to BLIP-2 [ 28] and OpenFlamingo [ 5], LLaV A accurately follows the user’s instructions,\ninstead of simply describing the scene. LLaV A offers a more comprehensive response than GPT-4.\nEven when merely asked to describe the image, LLaV A identifies atypical aspects of the image.\nSurprisingly, although LLaV A is trained with a small multimodal instruction-following dataset\n(∼80K unique images), it demonstrates quite similar reasoning results with multimodal GPT-4 on\nthese examples. Note that while these images are out-of-domain for LLaV A, LLaV A is still able\nto understand the scenes and follow the question instruction to provide a reasonable response. In\ncontrast, BLIP-2 and OpenFlamingo focus on describing the image, instead of following the user\ninstruction to answer in an appropriate manner.\nQuantitative Evaluation. To gain a systematic understanding of the performance of LLaV A, we\npropose a quantitative metric to measure the model’s instruction-following capability on multimodal\ndata. Inspired by [ 9], we leverage GPT-4 to measure the quality of generated responses. Specifically,\nwe create triplets consisting of image, ground-truth textual descriptions, and question. The candidate\nmodels ( e.g., LLaV A) predict the answers based on the question and the image. To provide an\napproximate theoretical upper bound , we create a reference prediction based on the question and\ntheground-truth textual descriptions, using the text-only GPT-4. After obtaining the responses from\nboth models, we feed the question, visual information (in the format of textual descriptions), and\nthe generated responses from both assistants, to the judge ( i.e.,text-only GPT-4). It evaluates the\nhelpfulness, relevance, accuracy, and level of detail of the responses from the assistants, and gives an\noverall score on a scale of 1 to 10, where a higher score indicates better overall performance. It is\nalso asked to provide a comprehensive explanation for the evaluation, for us to better understand the\n6Conversation Detail description Complex reasoning All\nFull data 83.1 75.3 96.5 85.1\nDetail + Complex 81.5 (-1.6) 73.3 (-2.0) 90.8 (-5.7) 81.9 (-3.2)\nConv + 5% Detail + 10% Complex 81.0 (-2.1) 68.4 (-7.1) 91.5 (-5.0) 80.5 (-4.4)\nConversation 76.5 (-6.6) 59.8 (-16.2) 84.9 (-12.4) 73.8 (-11.3)\nNo Instruction Tuning 22.0 (-61.1) 24.0 (-51.3) 18.5 (-78.0) 21.5 (-63.6)\nTable 4: Ablation on LLaV A-Bench (COCO) with different training data. We report relative scores\nw.r.t. a text-only GPT-4 model that uses ground truth image captions and bounding boxes as visual\ninput. We prompt GPT-4 with the answers from our model outputs and the answers by GPT-4\n(text-only), and let it compare between both responses and give a rating with an explanation.\nConversation Detail description Complex reasoning All\nOpenFlamingo [5] 19.3±0.5 19.0 ±0.5 19.1 ±0.7 19.1 ±0.4\nBLIP-2 [28] 54.6±1.4 29.1 ±1.2 32.9 ±0.7 38.1 ±1.0\nLLaV A 57.3±1.9 52.5 ±6.3 81.7 ±1.8 67.3 ±2.0\nLLaV A†58.8±0.6 49.2 ±0.8 81.4 ±0.3 66.7 ±0.3\nTable 5: Instruction-following capability comparison using relative scores on LLaV A-Bench (In-the-\nWild). The results are reported in the format of mean±std. For the first three rows, we report three\ninference runs. LLaV A performs significantly better than others.†For a given set of LLaV A decoding\nsequences, we evaluate by querying GPT-4 three times; GPT-4 gives a consistent evaluation.\nmodels. We report relative scores w.r.t. the text-only GPT-4 model that uses the textural ground truth\ndescription as visual input. We create two benchmarks to evaluate the model’s performance.\nLLaVA-Bench (COCO). We randomly select 30 images from COCO-Val-2014, and for each\nimage, we generate three types of questions (conversation, detailed description, complex reasoning)\nusing the proposed data generation pipeline in Sec. 3, totaling 90 questions. This benchmark studies\nthe model’s alignment behavior and capabilities with consistent visual inputs. We vary the training\ndatasets to study the effectiveness of different types of instruction-following data, and show the results\nin Table 4. First, with instruction tuning, the model’s ability of following user instructions improves\nsignificantly by over 50 points. Second, adding a small amount of detailed description and complex\nreasoning questions contributes to a considerable improvement of the model’s overall capability\nby 7 points. Furthermore, it also improves the model’s performance on conversational questions,\nsuggesting that improvements in reasoning capabilities complement conversational abilities. Finally,\nwe show that having all three types of data yields the best performance at 85.1%.\nLLaVA-Bench (In-the-Wild). To evaluate the model’s capability in more challenging tasks and\ngeneralizability to novel domains, we collect a diverse set of 24 images with 60 questions in total,\nincluding indoor and outdoor scenes, memes, paintings, sketches, etc., and associate each image\nwith a highly-detailed and manually-curated description and a proper selection of questions. We\ncompare LLaV A, BLIP, and OpenFlamingo in Table 5. Thanks to visual instruction tuning, LLaV A\nachieves significantly better performance compared with BLIP-2 (+29%) and OpenFlamingo (+48%).\nCompared to the text-only GPT-4 that has access to ground-truth labels, LLaV A achieves an impressive\n81.7% performance on complex reasoning questions, with an overall score of 67.3%.\nLimitations. This LLaV A-Bench (In-the-Wild) is designed to be challenging and to reveal a model’s\nweaknesses. We provide two examples with associated captions and questions in Table 6. For the\nramen example (left), to correctly answer the name of the restaurant, it requires the model to have\na large knowledge coverage and multilingual understanding capability; to correctly describe the\nside dishes, the model may need to retrieve relevant multimodal information from Internet. For the\nfridge example (right), perceiving the correct brand of the yogurt requires the model to process high\nresolution images and possess extensive knowledge coverage. We also observed an interesting failure\nof LLaV A, as it responds with yeswhen asked if strawberry-flavored yogurt is present, even though\nthe fridge contains only yogurt andstrawberries. This indicates that, at times, LLaV A perceives\nthe image as a “bag of patches”, failing to grasp the complex semantics within the image. We hope\nLLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work\nin developing more capable LMMs.\n7Challenging examples from LLaVA-Bench (In-the-Wild):\nICHIRAN Ramen [source] Filled fridge [source]\nAnnotation A close-up photo of a meal at ICHI-\nRAN. The chashu ramen bowl with\na spoon is placed in the center. The\nramen is seasoned with chili sauce,\nchopped scallions, and served with\ntwo pieces of chashu. Chopsticks are\nplaced to the right of the bowl, still in\ntheir paper wrap, not yet opened. The\nramen is also served with nori on the\nleft. On top, from left to right, the fol-\nlowing sides are served: a bowl of or-\nange spice (possibly garlic sauce), a\nplate of smoke-flavored stewed pork\nwith chopped scallions, and a cup of\nmatcha green tea.An open refrigerator filled with a variety of food\nitems. In the left part of the compartment, towards\nthe front, there is a plastic box of strawberries with a\nsmall bag of baby carrots on top. Towards the back,\nthere is a stack of sauce containers. In the middle\npart of the compartment, towards the front, there\nis a green plastic box, and there is an unidentified\nplastic bag placed on it. Towards the back, there is a\ncarton of milk. In the right part of the compartment,\ntowards the front, there is a box of blueberries with\nthree yogurts stacked on top. The large bottle of\nyogurt is Fage non-fat yogurt, and one of the smaller\ncups is Fage blueberry yogurt. The brand and flavor\nof the other smaller cup are unknown. Towards the\nback, there is a container with an unknown content.\nQuestion 1 What’s the name of the restaurant? What is the brand of the blueberry-flavored yogurt?\nQuestion 2 Describe this photo in detail. Is there strawberry-flavored yogurt in the fridge?\nTable 6: Challenging examples from LLaV A-Bench (In-the-Wild), we provide extremely-detailed\nannotation for each image for an accurate evaluation. Some questions require the model to extract\ndetails from high resolution image and to have a broad knowledge coverage.\n5.2 ScienceQA\nScienceQA [ 34] contains 21k multimodal multiple choice questions with rich domain diversity across\n3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training,\nvalidation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two\nrepresentative methods, including GPT-3.5 model ( text-davinci-002 ) with and without chain-\nof-thought (CoT), LLaMA-Adapter [ 59], as well as multimodal chain-of-thought (MM-CoT) [ 61],\nwhich is the current SoTA method on this dataset. For more baseline numbers, please see [34].\nThe results are reported in Table 7. For LLaV A, we use the visual features before the last layer, ask\nthe model to first predict reasons and then the answer, and train it for 12 epochs. It yields 90.92%\naccuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt\nGPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain\ncompared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails\nsimply because it reports that there is insufficient context such as images or plots. We consider two\nschemes to combine the outcomes from our model and GPT-4. (i)A GPT-4 complement . Whenever\nGPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97%\naccuracy, which is almost the same as applying our method alone. (ii)GPT-4 as the judge . Whenever\nGPT-4 and LLaV A produce different answers, we prompt GPT-4 again, asking it to provide its own\nfinal answer based on the question and two outcomes. The spirit is similar with CoT, but with the\nexternal knowledge from the other model. Surprisingly, this scheme is able to provide consistent\nimprovement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly,\nthe text-only GPT-4, which cannot process images, improves the overall performance of the model\non questions that have an image as context. This is because some of these questions do not actually\nrequire the image context for a correct answer. The GPT-4 judge can identify such cases and correct\nsome of the errors that LLaV A makes. See the example in Appendix. To the best of our knowledge,\n8MethodSubject Context Modality GradeAverageNAT SOC LAN TXT IMG NO G1-6 G7-12\nRepresentative & SoTA methods with numbers reported in the literature\nHuman [34] 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nGPT-3.5 [34] 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT [34] 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nLLaMA-Adapter [59] 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19\nMM-CoT Base [61] 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMM-CoT Large [61] 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nResults with our own experiment runs\nGPT-4†84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69\nLLaV A 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92\nLLaV A+GPT-4†(complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97\nLLaV A+GPT-4†(judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53\nTable 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC =\nsocial science, LAN = language science, TXT = text context, IMG = image context, NO = no context,\nG1-6 = grades 1-6, G7-12 = grades 7-12.†Text-only GPT-4, our eval. Our novel model ensembling\nwith the text-only GPT-4 consistently improves the model’s performance under all categories, setting\nthe new SoTA performance.\nthis is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage\nfuture research to explore more effective methods to leverage LLMs for model ensembling.\nVisual features Before Last\nBest variant 90.92 89.96 (-0.96)\nPredict answer first - 89.77 (-1.15)\nTraining from scratch 85.81 (-5.11) -\n7B model size 89.84 (-1.08) -\nTable 8: Design choice ablations (%). The differ-\nence with the best variant is reported in red text.Ablations. We ablate several design choices\non ScienceQA in Table 8. (i)Visual features .\nWe tried using the last layer feature from CLIP\nvision encoder, which yields 89.96% and is\n0.96% lower than the feature before the last\nlayer. We hypothesize that this is because\nCLIP’s last layer features may focus more on\nglobal and abstract image properties compared\nto the layer before it, which can focus more on\nlocalized properties that are useful for under-\nstanding specific image details. (ii)Chain-of-thought . To decide the order between the answer\nand reasoning process in the model prediction, we run both variants and observe that answer-first\nreports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach\n89.77% accuracy in 6 epochs, but no further improvement with more training. Training the model\nfor 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy\ncan largely improve convergence, but contributes relatively little to the final performance. (iii)\nPre-training . We skip pre-training and directly train on Science QA from scratch – performance drops\nto 85.81% accuracy. The 5.11% absolute degradation indicates the importance of our pre-training\nstage, in aligning multimodal features while preserving the vast pre-trained knowledge. (iv)Model\nsize. We keep all configurations the same as our best 13B model, and train a 7B model. This yields\n89.84% accuracy, which is 1.08% lower than 90.92%, demonstrating the importance of model scale.\n6 Conclusion\nThis paper demonstrated the effectiveness of visual instruction tuning. We presented an automatic\npipeline to create language-image instruction-following data, based on which we train LLaV A, a\nmultimodal model to follow human intent to complete visual tasks. It achieves the new SoTA\naccuracy when fine-tuned on ScienceQA, and excellent visual chat capabilities when fine-tuned\non multimodal chat data. Besides, we present the first benchmark to study multimodal instruction-\nfollowing capability. This paper is an initial step in visual instruction tuning, and mainly focuses on\nreal-life tasks. For more quantitative results of LLaV A on academic benchmarks, please refer to the\nimproved baselines with visual instruction tuning [ 32]. We hope our work can inspire future research\non building more capable multimodal models.\nAcknowledgements. We thank Baolin Peng and Pan Lu for valuable discussions on instruction-tuning\nlanguage models and Science QA, respectively. We thank the LLaMA team for giving us access\n9to their models, and open-source projects, including Alpaca and Vicuna. This work was supported\nin part by NSF CAREER IIS2150012, and Institute of Information & communications Technology\nPlanning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871,\nDevelopment of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and\n(No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient\nPre-training).\nReferences\n[1] Langchain. https://github.com/hwchase17/langchain , 2022. 2\n[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022. 2, 4\n[3]Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid,\nStephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2018. 2\n[4]Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\nlaboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021. 1\n[5]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 6, 7\n[6]Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instruct pix2pix: Learning to follow\nimage editing instructions. arXiv preprint arXiv:2211.09800 , 2022. 2\n[7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. 2\n[8]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In CVPR , 2021. 2\n[9]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 2, 4, 5, 6\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 2\n[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416 , 2022. 2\n[12] CVinW. Computer vision in the wild. https://github.com/\nComputer-Vision-in-the-Wild/CVinW_Readings , 2022. 1\n[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378 , 2023. 2\n[14] Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad\nRastegari, and Oncel Tuzel. Reinforce data, multiply impact: Improved model accuracy and\nrobustness with dataset reinforcement. arXiv preprint arXiv:2303.08983 , 2023. 2\n[15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-based text-to-image generation with human priors. ArXiv , abs/2203.13131,\n2022. 1\n10[16] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-\nlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends ®in\nComputer Graphics and Vision , 2022. 1\n[17] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for\ntext-annotation tasks. arXiv preprint arXiv:2303.15056 , 2023. 3\n[18] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. arXiv preprint arXiv:2211.11559 , 2022. 2\n[19] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a\ngeneric agent for vision-and-language navigation via pre-training. In CVPR , 2020. 2\n[20] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045 , 2023. 2\n[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan\nTaori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,\nAli Farhadi, and Ludwig Schmidt. Openclip. July 2021. If you use this software, please cite it\nas below. 1\n[22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 ,\n2022. 2\n[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan,\nand Ser-Nam Lim. Visual prompt tuning. In ECCV , 2022. 2\n[24] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images\nfor multimodal generation. arXiv preprint arXiv:2301.13823 , 2023. 2\n[25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and René Ranftl. Language-\ndriven semantic segmentation. ICLR , 2022. 1\n[26] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint arXiv:2309.10020 , 2023. 1\n[27] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang,\nPing Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEV ATER: A bench-\nmark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on\nDatasets and Benchmarks , 2022. 1\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 , 2023. 1, 2, 4, 6, 7\n[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu\nZhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image\npre-training. In CVPR , 2022. 1\n[30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint\narXiv:2301.07093 , 2023. 1\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV ,\n2014. 3\n[32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning, 2023. 9, 14\n11[33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499 , 2023. 1\n[34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering. Advances in Neural Information Processing Systems ,\n2022. 2, 5, 8, 9\n[35] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/ , 2023. 1, 2\n[36] OpenAI. Gpt-4 technical report, 2023. 1, 5, 6, 15\n[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems ,\n35:27730–27744, 2022. 2\n[38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith GPT-4. arXiv preprint arXiv:2304.03277 , 2023. 1, 4\n[39] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu,\nJiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for\nopen-vocabulary image classification. arXiv preprint arXiv: 2111.10050 , 2021. 1\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020 , 2021. 1, 2, 4\n[41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research , 2020. 2\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. ArXiv , abs/2204.06125, 2022. 1\n[43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. CVPR , pages 10674–10685, 2022.\n1\n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo\nLopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic\ntext-to-image diffusion models with deep language understanding. ArXiv , abs/2205.11487,\n2022. 1\n[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402 , 2022. 2\n[46] Dídac Surís, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128 , 2023. 2\n[47] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah\nMaestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,\nVladimir V ondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira,\nVladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home\nassistants to rearrange their habitat. In Advances in Neural Information Processing Systems\n(NeurIPS) , 2021. 2\n[48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023. 1, 4\n12[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 1\n[50] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language.\narXiv preprint arXiv:2205.14100 , 2022. 1\n[51] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560 , 2022. 2\n[52] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.\nBenchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv\npreprint arXiv:2204.07705 , 2022. 2\n[53] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671 , 2023. 2\n[54] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao.\nUnified contrastive learning in image-text-label space. CVPR , 2022. 1\n[55] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 2\n[56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive\nmodels for content-rich text-to-image generation. ArXiv , abs/2206.10789, 2022. 1\n[57] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong\nHu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for\ncomputer vision. arXiv preprint arXiv:2111.11432 , 2021. 1\n[58] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang,\nand Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv\npreprint arXiv:2303.08131 , 2023. 1\n[59] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199 , 2023. 2, 8, 9\n[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068 , 2022. 2\n[61] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi-\nmodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 , 2023.\n8, 9\n[62] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image\npretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 16793–16803, 2022. 1\n[63] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language.\narXiv preprint arXiv:2212.11270 , 2022. 1, 2\n13A Broader Impact\nThe broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks\nassociated with its deployment and release. Some considerations are unique to LLaV A due to its\nvisual nature, while others share similarities with existing instruction-following LLMs ( e.g., Alpaca,\nVicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues\nassociated with LLMs and vision encoders. In the following, we outline both the risks and mitigation\nstrategies in place for the release of this model.\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre-\ncautionary measures for LLaV A: (1) OpenAI Filter API for user input text to prevent harmful or\ninappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded\nuser images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful\nimage inputs.\nHallucination. Similar to LLMs, LLaV A might generate outputs that aren’t grounded in facts\nor input data. This raises concerns about inferences made, especially in critical applications ( e.g.,\nmedical).\nBiases. Bias can be transferred from the base models to LLaV A, both from the vision encoder\n(CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair\nrepresentations of diverse content.\nEnergy consumption. Though energy consumption is not a primary concern for LLaV A due to\na smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the\npretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\nEvaluation complexities. Assessing the performance of LLaV A is challenging as it involves both\nlanguage and visual tasks. Our evaluation benchmark covers several aspects, including accuracy,\nconcept coverage, reasoning ability, and creativity. However, additional aspects need consideration,\nsuch as the degree of visual content hallucination and fine-grained understanding of visual content.\nWhile text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its\nrobustness in different situations and capability to evaluate other unexplored aspects are subjects for\nfuture work.\nDespite these risks, we believe that the benefits of releasing LLaV A to the research community\noutweigh the potential harm. It allows for ongoing investigation and improvement of the model and\nengages the community in developing better mitigation strategies to address these concerns. Moreover,\nthe release of LLaV A can spur the development of new applications and research directions, ultimately\ncontributing to the progress and responsible deployment of foundation models in vision-language\ntasks.\nB More Results\nWe present more qualitative results of LLaV A to analyze its emergent behaviors and observed\nweaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the\nimproved baselines with visual instruction tuning [ 32]. In Table 9, LLaV A demonstrates a similar\nbehavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI,\nLLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on\na simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can\nfollow user’s instructions in a conversational style and provide detailed responses or creative writings.\nFurthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained\nLLM, as demonstrated in Fig. 4 and Fig. 5.\nOne interesting emergent behavior of LLaV A is that it is able to understand visual contents that\nare not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a\nheadshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never\nappears in the training data for either the visual feature alignment or visual instruction tuning stages of\n14LLaV A. LLaV A also demonstrates impressive OCR (optical character recognition) ability in Table 9\nand Fig. 2, which is rarely covered in our training data.\nWe hope these additional results and observations showcase the potential of LLaV A in various\napplication areas. In future work, it is important to investigate these emergent behaviors more\nthoroughly and to understand the underlying mechanisms that enable LLaV A to demonstrate such\ngeneralization abilities. This will pave the way towards building better LMMs, including enhancing\nrobustness, reducing biases, and improving the alignment and the scope of the learned vision-language\nrepresentations.\nVisual input example, Chicken Nugget Map:\nUser Can you explain this meme in detail?\nLLaV A The meme in the image is a creative and humorous take on food, with a focus on chicken\nnuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\nresemble the continents and islands on a world map. The meme playfully suggests that the\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\nactually chicken nugget versions of different places. The punchline of the meme appears\nin smaller text at the bottom, reading \"I mean, it’s not the real Earth, but how beautiful it\nis all is.\" This lighthearted and entertaining meme combines food and humor, using the\nimage of chicken nuggets to create a funny and imaginative representation of the world.\nUser Can you explain this meme?\nGPT-4 [36] This meme is a joke that combines two unrelated things: pictures of the earth from space\nand chicken nuggets. The text of the meme suggests that the image below is a beautiful\npicture of the earth from space. However, the image is actually of chicken nuggets\narranged to vaguely resemble a map of the world. The humor in this meme comes from\nthe unexpected juxtaposition of the text and the image. The text sets up an expectation of a\nmajestic image of the earth, but the image is actually something mundane and silly.\nUser Can you explain this meme in detail?\nBLIP-2 sometimes i just look at pictures of the earth from space and marvel how beautiful it is\nUser Can you explain this meme in detail?\nOpenFlamingo It’s a picture of a chicken nugget on the International Space Station.\nTable 9: Example prompt comparing LLaV A, GPT-4, BLIP-2, and OpenFlamingo’s visual reasoning\ncapabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user’s\ninstructions. LLaV A and GPT-4 both explain the meme and its humor, while GPT-4 produces a more\nconcise answer.\n15<!DOCTYPE html>\n<html>\n<head>\n<title>My Joke Website</title>\n<style>\nbody { background -color: white; font -family: Arial, sans -serif; }\n.container { display: flex; flex -direction: column; align -items: center; justify -content: center; height: 100vh; }\nh1 { font -size: 36px; margin -bottom: 20px; }\np { font -size: 20px; }\nbutton { background -color: blue; color: white; border: none; padding: 10px 20px; font -size: 18px; cursor: pointer; }\n</style>\n</head>\n<body>\n<div class=\"container\">\n<h1>My Joke Website</h1>\n<pid=\"punchline\" >Welcome to my joke website. Push the button to reveal the punchline!</p>\n<button onclick=\" revealPunchline ()\">Push me!</button>\n</div>\n<script>\nfunction revealPunchline () {\ndocument.getElementById (\"punchline\"). innerHTML = \"The punchline is: 'Why was the math book sad? Because it had too many \nproblems.'\";\n}\n</script>\n</body>\n</html>Turn this mock -up into an interactive website using html/ js, replace \nthe joke with a real joke.\nRendered website Rendered website\n(clicked the button)Place the generated HTML code into an HTML renderer to render the web page.User\nLLaVA\nFigure 2: LLaV A generates HTML/JS code for an interactive website based on user sketch inputs.\nThe interactive interface works after fixing a minor error ( in red ) in the generated output. There is\nroom for improvement in LLaV A’s output, such as splitting the joke and punchline into two rows,\nand only revealing the punchline upon button click, to better reflect the user’s intent.\n16With the variety of food items stored in the refrigerator, you can prepare several meals. For example, you can create a fruit salad using the \nstrawberries, blueberries, and carrots. Additionally, you can make a delicious smoothie using the fruits and vegetables, whic h would be a \nhealthy and nutritious snack or breakfast option. The milk and eggs can be used to create various dishes, such as a milk -based s moothie, a \nbaked casserole, or scrambled eggs. The juice can also be used to make a fresh fruit and juice mix or as a refreshing homemad e popsicle. The \npossibilities are vast, and the meals you can create will depend on your preferences and dietary needs.What are the meals that I can cook with these?\nCertainly! Here's a simple fruit salad recipe using the items in your refrigerator:\nIngredients:\n-2 cups fresh strawberries, sliced\n-1 cup fresh blueberries\n-1 large carrot, grated\n-Juice of half a lemon\n-A handful of chopped fresh parsley or mint\n-A few ice cubes\n-Optional: honey or maple syrup (adjust to taste)\nInstructions:\n1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix.\n2. Drizzle the juice of half a lemon over the fruit and vegetables to prevent them from oxidizing and add a refreshing citrus flavor.\n3. Sprinkle the chopped fresh parsley or mint over the fruit and gently toss once more.\n4. If desired, add a drizzle of honey or maple syrup for a touch of sweetness. Adjust the amount according to your taste pref erences.\n5. Cover the bowl and refrigerate the fruit salad for at least 30 minutes or up to a few hours. This allows the flavors to me ld and creates a \nmore refreshing salad.\n6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired.\nThis fruit salad recipe is perfect for a healthy snack or side dish, and with the variety of ingredients available in your re frigerator, it should be \neasy to put together quickly.I would like to do the fruit salad.  Show me the recipe.\nLLaVA\nLLaVA\nUser\nUser\nWhen visiting this peaceful location with a pier extending over a serene lake, surrounded by mountains, there are several thi ngsto be cautious \nabout. First, pay attention to the weather conditions, as sudden changes in weather, especially strong winds or storms, can c ause the water to \nbecome rough and pose risks to the stability of the pier. Second, be mindful of the structural integrity of the pier; avoid o verloading it with too \nmany people or excessive weight, as this could potentially damage the pier or cause it to collapse. Additionally, be aware of the surrounding \nwildlife and respect their space; do not harm or disturb them in any way. Finally, if you decide to go fishing, ensure that y ou adhere to any local \nfishing regulations and use appropriate fishing techniques to keep the lake ecosystem healthy and maintain the balance of the food chain.\nWhat are the things I should be cautious about when I visit here?\nTraveling to this picturesque location with a serene lake and mountains in the background was a refreshing and rejuvenating e xperience. The \npier extending over the calm water provided an excellent vantage point from which to appreciate the breathtaking scenery. As I stood on the \npier, I couldn't help but feel a deep connection with nature, taking a moment to soak in the peaceful atmosphere.\nOne of the highlights of my trip was undoubtedly the fishing experience. The area is teeming with life, and I was thrilled to land a sizeable \ncatch, despite the modest size of the lake. The joy of fishing in such a beautiful setting was truly rewarding, and it allowe d me to take a break \nfrom my busy life and truly immerse myself in the tranquility of nature.\nWhile exploring the surrounding area, I came across a family of birds that had built a nest close to the pier, adding to the charm of the \nlocation. Interacting with the wildlife and observing their behavior was an enriching experience that I'll cherish for years to come.\nIn conclusion, this idyllic destination provided me with an opportunity to unwind and recharge, surrounded by mesmerizing nat ural beauty and \nthe gentle sounds of water and wildlife. If you're looking for a serene escape from the hustle and bustle of everyday life, t hispier overlooking \nthe lake is the perfect place to find solace and reconnect with nature.Compose an engaging travel blog post about my trip to this place, highlighting the beautiful \nsceneries, joyful experiences, and include possible anecdotes.\nUser\nUserLLaVA\nLLaVA\nFigure 3: LLaV A is capable of recognizing the visual content following the user’s intent, without\ndirectly prompting for visual recognition. It also provides a detailed response when prompted with a\nfollow-up request, and the generated response is closely related to the provided visual content.\n17In the image, a young woman is leaning on a man in front of a beautiful sunset, with both of them standing on a boat. They appear to be \nenjoying a romantic moment together. The scene suggests that they could be depicted as the characters Jack and Rose from the popular movie \nTitanic. In the movie, their romance blossoms aboard the fictional ocean liner Titanic, with the pair ultimately sharing a pa ssionate kiss as the \nship sinks into the ocean. The ending of the movie is known for its emotional and heartfelt scenes, where the characters are forced to say \ngoodbye to each other as they are separated by the sinking ship. The image evokes a sense of romance and nostalgia, paying tr ibute to the \niconic love story portrayed in the movie Titanic.What's the ending of this movie?\nLLaVA\nUserFigure 4: LLaV A relates the movie scenes to the textual knowledge from the pretrained LLM.\nThe painting depicts a dog in a humorous situation, where it appears to be wearing a woman's dress and standing in front of a landscape. \nAlthough the artist of the painting is not explicitly identified, it could be a reference to the famous artist Da Vinci, know n for his artistic works, \nincluding the Mona Lisa. The painting is likely a creative and comical take on the traditional portrait style, replacing a hu man figure with a dog \ndressed in a woman's clothing, making it an interesting and amusing visual piece.Do you know who drew this painting?\nThe painting depicts a woman, commonly believed to be Mona Lisa, the famous artwork by Leonardo da Vinci. It is a portrait pa inting that \nshowcases the woman's enigmatic smile and has become one of the most famous and iconic art pieces in the world. The original work is \ndisplayed in the Louvre Museum in Paris, and it is known for its intricate details, use of oil paint, and the artist's innova tive techniques that \ncontributed to its enduring appeal and mystery.Do you know who drew this painting?\nStart a new conversation, and the history is cleared.User\nUserLLaVA\nLLaVA\nFigure 5: LLaV A recognizes the famous art work, Mona Lisa, by Leonardo da Vinci. When we start\na new conversation, it also explains the humourous artwork created on the web, mimicking the Mona\nLisa.\n18The name of the man in the photo is Elon Musk.What is the name of the man in the photo?\nThe name of the man in the photo is Elon Musk.What is the name of the man in the photo?\nStart a new conversation, and the history is cleared.User\nUserLLaVA\nLLaVA\nFigure 6: An interesting emergent behavior of LLaV A is its ability to recognize Elon Musk both in a\nheadshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained\nCLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon\nMusk never appears in the training data for either the visual feature alignment or visual instruction\ntuning stages of LLaV A, which indicates that the base language model generalizes to unseen visual\nconcepts.\nQuestion:\nWhich material is this rocking chair made of?\nOptions: (A) wood (B) silk\nLLaVA answer:\nLECTURE: A material is a type of matter.\nWood, glass, metal, and plastic are common materials.\nSome objects are made of just one material.\nMost nails are made of metal.\nOther objects are made of more than one material.\nThis hammer is made of metal and wood.\nSOLUTION: Look at the picture of the rocking chair. The rocking chair is made of two different\nmaterials. The legs are made of wood, and the back and seat are made of silk.\nThe answer is B.\nGPT-4 (text-only) answer:\nThe answer is A. BECAUSE: Rocking chairs are typically made of sturdy materials that can support\nthe weight of a person and withstand the rocking motion. Wood is a common material used for\nmaking rocking chairs due to its strength, durability, and natural appearance.\nGPT-4 (text-only) judge:\nSince there is no context or image to refer to, I cannot independently analyze the rocking chair in\nquestion. However, I can try to analyze the reasoning given by the assistants.\nAssistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are\nmade of wood, and the back and seat are made of silk, implying that it’s a combination of two\nmaterials. However, this response seems less plausible, as silk is not a common material used for\nthe construction of rocking chairs due to issues with stability and durability. Typically, one would\nexpect the frame, legs, and seat or back to be made of wood, with cushions and padding made of\nfabrics, such as cotton or polyester.\nAssistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common\nmaterial used for making rocking chairs due to its strength, durability, and natural appearance. This\nreasoning stands true and is based on common knowledge.\nGiven the information available, the most plausible answer is that the rocking chair is made of wood,\nas suggested by Assistant 1. The answer is A.\nTable 10: One example on how the text-only GPT-4 acts as a judge to ensemble the predictions from\nLLaV A and a text-only GPT-4, and gives a correct final answer.\n19C Training Details\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\nWe train all models with 8 ×A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\nD Assets\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\n1. Source Code: link\n2. README: link\n3. Instructions to launch the demo: link\n4. All prompts and few shot examples for querying GPT-4: link\n5. LLaV A-Instruct-158K: link\n6. LLaV A-Bench: COCO, In-The-Wild\n7.Model checkpoints. The size of the model checkpoints after compression is 25GB, which\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\nthe public, or upon request with reviewers for this submission.\nE Data\nInstructions for brief image description. The list of instructions used to briefly describe the image\ncontent are shown in Table 11. They present the same meaning with natural language variance.\n• \"Describe the image concisely.\"\n• \"Provide a brief description of the given image.\"\n• \"Offer a succinct explanation of the picture presented.\"\n• \"Summarize the visual content of the image.\"\n• \"Give a short and clear explanation of the subsequent image.\"\n• \"Share a concise interpretation of the image provided.\"\n• \"Present a compact description of the photo’s key features.\"\n• \"Relay a brief, clear account of the picture shown.\"\n• \"Render a clear and concise summary of the photo.\"\n• \"Write a terse but informative summary of the picture.\"\n• \"Create a compact narrative representing the image presented.\"\nTable 11: The list of instructions for brief image description.\nInstructions for detailed image description. The list of instructions used to describe the image\ncontent in detail are shown in Table 12. They present the same meaning with natural language\nvariance.\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\nthan3, as they are usually rare combinations concept and attributes that has already been covered\n20• \"Describe the following image in detail\"\n• \"Provide a detailed description of the given image\"\n• \"Give an elaborate explanation of the image you see\"\n• \"Share a comprehensive rundown of the presented image\"\n• \"Offer a thorough analysis of the image\"\n• \"Explain the various aspects of the image before you\"\n• \"Clarify the contents of the displayed image with great detail\"\n• \"Characterize the image using a well-detailed description\"\n• \"Break down the elements of the image in a detailed manner\"\n• \"Walk through the important details of the image\"\n• \"Portray the image with a rich, descriptive narrative\"\n• \"Narrate the contents of the image with precision\"\n• \"Analyze the image in a comprehensive and detailed manner\"\n• \"Illustrate the image through a descriptive explanation\"\n• \"Examine the image closely and share its details\"\n• \"Write an exhaustive depiction of the given image\"\nTable 12: The list of instructions for detailed image description.\nby other captions. We then start from the noun-phrases with lowest remaining frequency, add the\ncaptions that contain this noun-phrase to the candidate pool. If the frequency of the noun-phrase\nis larger than 100, we randomly choose a subset of size 100out of all its captions. This results in\naround 595K image-text pairs.\nThe comparison of noun-phrase statistics before and after filtering CC3M is shown in Figure 7. The\nfiltered dataset shows a good coverage of concepts whose frequency is higher from 3, but with a\nsmaller number of image-text pairs.\n0 10000 20000 30000 40000 50000\nUnique noun-phrases (ordered by frequency in the descending order)101103105FrequencyCC3M: 108182\nCC3M (Filtered):  31423\nFigure 7: Comparison of noun-phrase statistics before and after filtering CC3M. The total number of\nunique noun-phrases are reported in the legend.\nF Prompts\nThe prompt used to generate image-based conversation from ChatGPT/GPT-4 is shown in Table 13.\n21messages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are\nseeing a single image. What you see are provided with five sentences, describing the same image you\nare looking at. Answer all questions as you are seeing the image.\nDesign a conversation between you and a person asking about this photo. The answers should be in a\ntone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions\nand give corresponding answers.\nInclude questions asking about the visual content of the image, including the object types, counting\nthe objects, object actions, object locations, relative positions between objects , etc. Only include\nquestions that have definite answers:\n(1) one can see the content in the image that the question asks about and can answer confidently;\n(2) one can determine confidently from the image that it is not in the image. Do not ask any question\nthat cannot be answered confidently.\nAlso include complex questions that are relevant to the content in the image, for example, asking\nabout background knowledge of the objects in the image, asking to discuss about events happening in\nthe image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering\ncomplex questions. For example, give detailed examples or reasoning steps to make the content more\nconvincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"}\n]\nfor sample in fewshot_samples:\nmessages.append({\"role\":\"user\", \"content\":sample[‘context’]})\nmessages.append({\"role\":\"assistant\", \"content\":sample[‘response’]} )\nmessages.append({\"role\":\"user\", \"content\":‘\\n’.join(query)})\nTable 13: For each query, we illustrate the prompt construction process for ChatGPT/GPT-4 to\ncollect query[‘response’] from query[‘context’] , using few-shot in-context-learning, where\nexamples are from fewshot_samples , each example including input sample[‘context’] and\noutput sample[‘response’] . Note that messages is the final prompt. In this example, we provide\nthe prompt used to generate the conversation response, please see also see its in-context-learning\nexamples in Table 15 and Table 16 for details. We recommend readers to check out the codebase for\nthe prompts to generated two other types of responses, including detailed decription and complex\nreasoning.\n22Context type 1: Captions\nA group of people standing outside of a black vehicle with various luggage.\nLuggage surrounds a vehicle in an underground parking area\nPeople try to fit all of their luggage in an SUV .\nThe sport utility vehicle is parked in the public garage, being packed for a trip\nSome people with luggage near a van that is transporting it.\nContext type 2: Boxes\nperson: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233,\n0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692],\nsuitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363,\n0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254,\n0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626]\nResponse type 1: conversation\nQuestion: What type of vehicle is featured in the image?\nAnswer: The image features a black sport utility vehicle (SUV).\nQuestion: Where is the vehicle parked?\nAnswer: The vehicle is parked in an underground parking area, likely in a public garage.\nQuestion: What are the people in the image doing?\nAnswer: The people are trying to fit all of their luggage into the SUV , likely preparing for a trip.\nResponse type 2: detailed description\nThe image is an underground parking area with a black sport utility vehicle (SUV) parked. There\nare three people in the scene, with one person standing closer to the left side of the vehicle, another\nperson in the middle, and the third person on the right side. They are all working together to pack\ntheir luggage into the SUV for a trip.\nIn the parking area, various luggage items are scattered around the vehicle. There are two backpacks,\none located near the left rear wheel and the other closer to the right side of the vehicle. Additionally,\nthere are two suitcases, one on the right side of the car and another further away near the center of\nthe parking area. A bicycle can also be seen on the left side of the vehicle.\nOther cars are parked around the main SUV , with one car positioned behind it and slightly to the left,\nanother behind and slightly to the right, and the third car further behind on the right side.\nResponse type 3: complex reasoning\nQuestion: What challenges do these people face?\nAnswer: In the image, a group of people is standing outside a black SUV in a parking area,\nsurrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\nchallenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks\nto be packed, which suggests that the group has a significant amount of belongings to accommodate.\nThey might have to strategize and arrange the luggage efficiently to ensure that everything fits\nproperly into the vehicle. Additionally, they need to consider the comfort of the passengers and\nvisibility while driving, so the placement of the luggage must not obstruct the driver’s view or make\nthe passengers uncomfortable during the trip.\nTable 14: One example to illustrate the instruction-following data. The top block shows the contexts\nsuch as captions and boxes used to prompt GPT, and the bottom block shows the three types of\nresponses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\n23Captions\nThere is a movie theater that displays the show times above the doors.\nA red fire hydrant is deep in the snow.\nThe fire hydrant is in the snow near a recently plowed sidewalk.\nThis city has had a very hard winter with snow.\nA hotel for dogs in the snow in winter.\nConversation\nQuestion:\nWhat color is the fire hydrant in the image?\n===\nAnswer:\nThe fire hydrant in the image is red.\n===\nQuestion:\nIs the fire hydrant fully covered by snow or partially visible?\n===\nAnswer:\nThe fire hydrant is partially visible, with a significant portion of it covered by the deep snow.\n===\nQuestion:\nWhat is the condition of the sidewalk near the fire hydrant?\n===\nAnswer:\nThe sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area.\n===\nQuestion:\nWhat does the image suggest about the winter season in the city?\n===\nAnswer:\nThe image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation\nseen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for\nits residents.\nFor instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses\nsafety concerns. The recently plowed sidewalk implies that the city’s maintenance crews have been actively working to keep the\nwalkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous\nsnow removal efforts.\nFurthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and\nincreased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face\nchallenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as\nroofs and power lines, increasing the likelihood of structural damage or power outages.\nIn conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced\na particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its\nresidents and infrastructure.\nTable 15: One example used in in-context-learning to construct visual conversation data.\n24Captions\nA man is skiing in the open snow covered hills\nA skier is making his way into the snow.\nA skier on the foothills of a huge mountain range.\nA skier looks at mountains while standing near a trail sign.\na single person skiing by an area with a lot of bushes\nConversation\nQuestion:\nWhat is the skier doing?\n===\nAnswer:\nThe skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They\nare also standing near a trail sign, which indicates their location on the mountain.\n===\nQuestion:\nAre there any other people or skiers visible in the image?\n===\nAnswer:\nNo, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes.\n===\nQuestion:\nWhat kind of skiing is the person doing? Cross-country or downhill?\n===\nAnswer:\nBased on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or\ndownhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that\nthe skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts,\nwhereas cross-country skiing often occurs on relatively flatter terrain with marked trails.\n===\nQuestion:\nCan you explain cross-country and downhill skiing a bit more?\n===\nAnswer:\nCross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain\npreferences.\n1. Cross-Country Skiing:\nAlso known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles.\nIt is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use\ntheir own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance.\nThere are two main techniques in cross-country skiing:\n- Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis\nparallel to each other. The skier uses their poles to propel themselves forward.\n- Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It\nrequires a smooth, groomed surface and is typically faster than the classic technique.\nCross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used\nin downhill skiing, allowing for better ankle movement and control.\n2. Downhill Skiing:\nAlso known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and\ncontrol. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and\neven jumps.\nDownhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing.\nEach discipline has its own set of rules, courses, and ski equipment.\nDownhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The\nboots are more rigid to provide better support and control during high-speed descents and sharp turns.\nIn summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while\ndownhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require\nspecialized equipment and techniques, but they offer different experiences and challenges to participants.\nTable 16: One example used in in-context-learning to construct visual conversation data.\n25"}